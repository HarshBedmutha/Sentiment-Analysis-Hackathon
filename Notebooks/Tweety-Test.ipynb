{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import nltk\n",
    "import string\n",
    "import seaborn as sns\n",
    "from nltk.stem.porter import * \n",
    "from wordcloud import WordCloud\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Data Science\\\\Jupyter Notebook\\\\GitHackathon\\\\Notebooks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/raw/train.csv')\n",
    "test = pd.read_csv('../data/raw/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1701</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of tou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1851</td>\n",
       "      <td>Learning ab Google doodles! All doodles should...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2689</td>\n",
       "      <td>one of the most in-your-face ex. of stealing t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4525</td>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3604</td>\n",
       "      <td>Line outside the Apple store in Austin waiting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet  sentiment\n",
       "0      1701  #sxswnui #sxsw #apple defining language of tou...          1\n",
       "1      1851  Learning ab Google doodles! All doodles should...          1\n",
       "2      2689  one of the most in-your-face ex. of stealing t...          2\n",
       "3      4525  This iPhone #SXSW app would b pretty awesome i...          0\n",
       "4      3604  Line outside the Apple store in Austin waiting...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7506</td>\n",
       "      <td>Audience Q: What prototyping tools do you use?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7992</td>\n",
       "      <td>At SXSW? Send Your Best Photos &amp;amp; Videos to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>247</td>\n",
       "      <td>@mention  and here's a pic of you winning your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7688</td>\n",
       "      <td>Google Marissa Mayer: mobile phone as a cursor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3294</td>\n",
       "      <td>#SXSW Google maps is even cooler than I thought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet\n",
       "0      7506  Audience Q: What prototyping tools do you use?...\n",
       "1      7992  At SXSW? Send Your Best Photos &amp; Videos to...\n",
       "2       247  @mention  and here's a pic of you winning your...\n",
       "3      7688  Google Marissa Mayer: mobile phone as a cursor...\n",
       "4      3294    #SXSW Google maps is even cooler than I thought"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "counts = train.sentiment.value_counts(normalize=True) * 100\n",
    "sns.barplot(x=counts.index, y=counts, ax=ax)\n",
    "ax.set_xticklabels(['Negative', 'Neutral', 'Positive', 'Cant Tell'])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    4311\n",
      "2    2382\n",
      "0     456\n",
      "3     125\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "Predicting only 2 = 32.75% accuracy\n"
     ]
    }
   ],
   "source": [
    "counts = train.sentiment.value_counts()\n",
    "print(counts)\n",
    "\n",
    "print(\"\\nPredicting only 2 = {:.2f}% accuracy\".format(counts[2] / sum(counts) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets = train[train.sentiment == 0].tweet\n",
    "nue_tweets = train[train.sentiment == 1].tweet\n",
    "pos_tweets = train[train.sentiment == 2].tweet\n",
    "ct_tweets = train[train.sentiment == 3].tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StopWord Removal\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    " \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import TextBlob\n",
    " \n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    " \n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "def lemmatize_sentence(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    nltk_tagged = nltk.pos_tag(tokenizer.tokenize(sentence))  \n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "          if (word not in stopwords_english and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "                #tweet = str(TextBlob(word).correct())                \n",
    "                if tag is None:\n",
    "                    #if there is no available tag, append the token as is\n",
    "                    lemmatized_sentence.append(word)\n",
    "                else:        \n",
    "                    #else use the tag to lemmatize the token\n",
    "                    lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)    \n",
    "    \n",
    "def clean_tweets(tweet):    \n",
    "    # remove numbers\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'\\d+', '', tweet)\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^rt[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub('[^a-zA-Z]', ' ', tweet)\n",
    "    return lemmatize_sentence(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cTrain = train.copy()\n",
    "#print(clean_tweets('I am loving it'))\n",
    "cTrain['lemma_tweet'] = cTrain.apply(lambda row: clean_tweets(str(row['tweet'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    sxswnui define language touch different dialec...\n",
       "1    learn ab doodle doodle light funny amp innovat...\n",
       "2      one face ex steal show yrs rt school mkt expert\n",
       "3    app would b pretty awesome crash every min ext...\n",
       "4                             line outside austin wait\n",
       "Name: lemma_tweet, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rTrain = cTrain.copy()\n",
    "\n",
    "freq = pd.Series(' '.join(rTrain['lemma_tweet']).split()).value_counts()[:10]\n",
    "freq\n",
    "freq = list(freq.index)\n",
    "rTrain['lemma_tweet'] = rTrain['lemma_tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "freq = pd.Series(' '.join(rTrain['lemma_tweet']).split()).value_counts()[-10:]\n",
    "freq\n",
    "freq = list(freq.index)\n",
    "rTrain['lemma_tweet'] = rTrain['lemma_tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "rTrain['lemma_tweet'].head()\n",
    "\n",
    "# rTrain.to_csv('../data/iter1/rTrain.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cTrain.head(50)\n",
    "\n",
    "#freq = pd.Series(' '.join(cTrain['lemma_tweet']).split()).value_counts()[:10]\n",
    "#freq\n",
    "\n",
    "cTrain['sentiment_val'] = cTrain['lemma_tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "cTrain[['tweet','sentiment','sentiment_val']].head()\n",
    "cTrain.to_csv('../data/iter1/cTrain.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\shank\\anaconda3\\lib\\site-packages (from textblob) (3.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\shank\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7274 entries, 0 to 7273\n",
      "Data columns (total 4 columns):\n",
      "tweet_id       7274 non-null int64\n",
      "tweet          7273 non-null object\n",
      "sentiment      7274 non-null int64\n",
      "lemma_tweet    7274 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 227.4+ KB\n"
     ]
    }
   ],
   "source": [
    "cTrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7271 entries, 0 to 7273\n",
      "Data columns (total 6 columns):\n",
      "Unnamed: 0       7271 non-null int64\n",
      "tweet_id         7271 non-null int64\n",
      "tweet            7271 non-null object\n",
      "sentiment        7271 non-null int64\n",
      "lemma_tweet      7271 non-null object\n",
      "sentiment_val    7271 non-null float64\n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 397.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#cTrain_org = cTrain.copy()\n",
    "#cTrain = rTrain.copy()\n",
    "cTrain = pd.read_csv('../data/iter1/rTrain.csv')\n",
    "cTrain.dropna(subset=['lemma_tweet'],inplace=True)\n",
    "cTrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7271x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 43612 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(cTrain['lemma_tweet'])\n",
    "train_bow\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7271x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 38572 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word', stop_words= 'english', ngram_range=(1,1))\n",
    "train_vect = tfidf.fit_transform(cTrain['lemma_tweet'])\n",
    "\n",
    "train_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7274, 1000) (7274, 1000) (7274, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train_vect.shape, train_bow.shape, cTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-aceed8c397e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_vect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_vect' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X = train_vect\n",
    "y = cTrain['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    " \n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators=100, random_state=0)  \n",
    "text_classifier.fit(X_train, y_train) \n",
    "predictions = text_classifier.predict(X_test)\n",
    " \n",
    "print(confusion_matrix(y_test,predictions))  \n",
    "print(classification_report(y_test,predictions))  \n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_optimal_k(X_train,y_train, myList):\n",
    "   \n",
    "    # empty list that will hold cv scores\n",
    "    cv_scores = []\n",
    "\n",
    "    # split the train data set into cross validation train and cross validation test\n",
    "    X_tr, X_cv, y_tr, y_cv = train_test_split(X_train, y_train, test_size=0.3)\n",
    "\n",
    "    for i in myList:\n",
    "        nb = MultinomialNB(alpha = i)\n",
    "        model = nb.fit(X_tr, y_tr)\n",
    "\n",
    "        # predict the response on the crossvalidation train\n",
    "        pred = model.predict(X_cv)\n",
    "\n",
    "        # evaluate CV accuracy\n",
    "        acc = accuracy_score(y_cv, pred, normalize=True)\n",
    "        cv_scores.append(acc)\n",
    "        \n",
    "    # changing to misclassification error\n",
    "    MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "    # determining best alpha\n",
    "    optimal_alpha = myList[MSE.index(min(MSE))]\n",
    "    print('\\nThe optimal alpha is ', optimal_alpha)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(myList,MSE,color='blue', linestyle='dashed', marker='o',\n",
    "             markerfacecolor='red', markersize=10)\n",
    "    plt.title('Error Rate vs. alpha Value')\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('Error Rate')\n",
    "\n",
    "    print(\"the misclassification error for each k value is : \", np.round(MSE,3))\n",
    "    \n",
    "    return optimal_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_bow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fb7505b32e3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfinal_train_bow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_bow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_train_bow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_bow' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "final_train_bow = StandardScaler(with_mean=False).fit_transform(train_bow)\n",
    "X = final_train_bow\n",
    "y = cTrain['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "myList = np.arange(0.00001, 0.001, 0.00005) #list(range(1,200))\n",
    "optimal_alpha = find_optimal_k(X_train ,y_train,myList)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB(optimal_alpha)\n",
    "model = nb.fit(X_train, y_train)\n",
    "predictions = nb.predict(X_test)\n",
    " \n",
    "print(confusion_matrix(y_test,predictions))  \n",
    "print(classification_report(y_test,predictions))  \n",
    "print(accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rTrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ed3bf1769dd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m }\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrTrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_tweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrTrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rTrain' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': [1, 1e-1, 1e-2]\n",
    "}\n",
    "\n",
    "X = rTrain.lemma_tweet\n",
    "y = rTrain.sentiment\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(rTrain, rTrain.lemma_tweet, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "clf = GridSearchCV(text_clf, tuned_parameters, cv=10, scoring='accuracy')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(classification_report(y_test, clf.predict(X_test), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-7c16248a2f4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    315\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m                 **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    318\u001b[0m             \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1058\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    350\u001b[0m                                                tokenize)\n\u001b[0;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 352\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-42ca9cd5e0b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mclean_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_tweets' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
