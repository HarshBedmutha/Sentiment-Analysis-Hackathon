{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for WittyWicky Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WittyWicky Inc. is a consulting firm that designs brand strategy for a lot of product startups. Their modus operandi is to gain the pulse of competing products and the associated sentiment from social media. Social media has profound impact in capturing the potential customers and thus there are a lot of consulting firms that operate in the digital strategy space. Whether it is to design a marketing campaign or look at the effect of marketing campaigns on user engagement or sentiment, it is a very valuable tool.\n",
    "\n",
    "Manual assessment of sentiment is very time consuming and automatic sentiment analysis would deliver a lot of value. As a team of data scientists consulting for WittyWicky Inc., you are now responsible for meeting their business outcomes.\n",
    "\n",
    "#### Problem Statement\n",
    "Twitter has now become a useful way to build one's business as it helps in giving the brand a voice and a personality. The platform is also a quick, easy and inexpensive way to gain valuable insight from the desired audience. Identifying the sentiments about the product/brand can help the business take better actions.\n",
    "\n",
    "You have with you evaluated tweets about multiple brands. The evaluators(random audience) were asked if the tweet expressed positive, negative, or no emotion towards a product/brand and labelled accordingly.\n",
    "\n",
    "### Dataset Description\n",
    "This dataset contains around 7k tweet text with the sentiment label.\n",
    "\n",
    "The file train.csv has 3 columns\n",
    "\n",
    "tweet_id - Unique id for tweets. tweet - Tweet about the brand/product sentiment - 0: Negative, 1: Neutral, 2: Positive, 3: Can't Tell\n",
    "\n",
    "#### Evaluation Metric\n",
    "We will be using ‘weighted’ F1-measure as the evaluation metric for this competition. For more information on the F1-metric refer to https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "\n",
    "#### Submission format\n",
    "Submission file should have two columns, one for tweet_id and sencond for sentiment [0: Negative, 1: Neutral, 2: Positive, 3: Can't Tell]. A sample submission file has also been attached for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "#Display all the columns \n",
    "pd.set_option('display.max_columns',None)\n",
    "# Display full length the column\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.stem.porter import *\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../../data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../../data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7269</th>\n",
       "      <td>3343</td>\n",
       "      <td>@mention Google plze Tammi.  I'm in middle of #SXSW craziness and everything is soooooo busy!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>5334</td>\n",
       "      <td>RT @mention ÷¼ Are you all set? ÷_ {link} ÷_ #edchat #musedchat #sxsw #sxswi #newTwitter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7271</th>\n",
       "      <td>5378</td>\n",
       "      <td>RT @mention Aha! Found proof of lactation room, excuse me, &amp;quot;Mother's Room,&amp;quot; brought to you by Google, at last year's #SXSW. {link}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7272</th>\n",
       "      <td>2173</td>\n",
       "      <td>We just launched our iPad app at #SXSW! Get all the details + the first edition FREE: {link}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7273</th>\n",
       "      <td>3162</td>\n",
       "      <td>The next fin serv battle is vs Apple, GOOG, Mobile operators. They have consumer loyalty and tons of cash (vs. Banks) #bankinnovate #SXSW</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id  \\\n",
       "7269  3343       \n",
       "7270  5334       \n",
       "7271  5378       \n",
       "7272  2173       \n",
       "7273  3162       \n",
       "\n",
       "                                                                                                                                             tweet  \\\n",
       "7269  @mention Google plze Tammi.  I'm in middle of #SXSW craziness and everything is soooooo busy!                                                  \n",
       "7270  RT @mention ÷¼ Are you all set? ÷_ {link} ÷_ #edchat #musedchat #sxsw #sxswi #newTwitter                                                    \n",
       "7271  RT @mention Aha! Found proof of lactation room, excuse me, &quot;Mother's Room,&quot; brought to you by Google, at last year's #SXSW. {link}   \n",
       "7272  We just launched our iPad app at #SXSW! Get all the details + the first edition FREE: {link}                                                   \n",
       "7273  The next fin serv battle is vs Apple, GOOG, Mobile operators. They have consumer loyalty and tons of cash (vs. Banks) #bankinnovate #SXSW      \n",
       "\n",
       "      sentiment  \n",
       "7269  1          \n",
       "7270  1          \n",
       "7271  1          \n",
       "7272  1          \n",
       "7273  1          "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic exploration of train data to check labels\n",
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>1550</td>\n",
       "      <td>@mention @mention @mention Hmmm....how fast can #apple build a new store in time for #sxsw  {link}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>1933</td>\n",
       "      <td>Samsung Galaxy S II Appears At FCC And Team Android #SXSW Party {link} via @mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>9052</td>\n",
       "      <td>@mention You could buy a new iPad 2 tmrw at the Apple pop-up store at #sxsw: {link}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>4219</td>\n",
       "      <td>Wow very long queue of people at apple pop up store now, some have bought 3 iPads! #sxsw@mention Room#NokiaConnects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>7210</td>\n",
       "      <td>Privacy Could Headline Google Circles Social Network Reveal Later Today [Social Networks] {link} #ACLU #GoogleCircles #SXSW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id  \\\n",
       "1814  1550       \n",
       "1815  1933       \n",
       "1816  9052       \n",
       "1817  4219       \n",
       "1818  7210       \n",
       "\n",
       "                                                                                                                            tweet  \n",
       "1814  @mention @mention @mention Hmmm....how fast can #apple build a new store in time for #sxsw  {link}                           \n",
       "1815  Samsung Galaxy S II Appears At FCC And Team Android #SXSW Party {link} via @mention                                          \n",
       "1816  @mention You could buy a new iPad 2 tmrw at the Apple pop-up store at #sxsw: {link}                                          \n",
       "1817  Wow very long queue of people at apple pop up store now, some have bought 3 iPads! #sxsw@mention Room#NokiaConnects          \n",
       "1818  Privacy Could Headline Google Circles Social Network Reveal Later Today [Social Networks] {link} #ACLU #GoogleCircles #SXSW  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic exploration of test data to check labels\n",
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id     int64 \n",
       "tweet        object\n",
       "sentiment    int64 \n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data types of the features\n",
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data distribution of target variable\n",
    "# class_count = df_train['sentiment'].value_counts()\n",
    "# class_count\n",
    "\n",
    "# # The sentiment labels are:\n",
    "\n",
    "# 0: Negative, 1: Neutral, 2: Positive, 3: Can't Tell\n",
    "\n",
    "# #### Bar Plot of value counts\n",
    "\n",
    "# x = np.array(class_count.index)\n",
    "# y = np.array(class_count.values)\n",
    "# plt.figure(figsize=(10,5))\n",
    "# sns.barplot(x,y)\n",
    "# plt.xlabel('Sentiment')\n",
    "# plt.ylabel('Number of tweets')\n",
    "\n",
    "# # Number of tweets in each Dataset\n",
    "# print('Number of sentences in training set:',len(df_train['tweet_id'].unique()))\n",
    "# print('Number of sentences in test set:',len(df_test['tweet_id'].unique()))\n",
    "# # print('Average words per sentence in train:',df_train.groupby('tweet_id')['sentiment'].count().mean())\n",
    "# # print('Average words per sentence in test:',df_test.groupby('tweet_id')['sentiment'].count().mean())\n",
    "\n",
    "# # Shape of the Datasets\n",
    "# df_train.shape, df_test.shape\n",
    "\n",
    "# #### Using Word Clouds to see the higher fequency words from each sentiment\n",
    "\n",
    "\n",
    "# # # stopwords = set(STOPWORDS)\n",
    "# # # def show_wordcloud(data, title = None):\n",
    "# # #     wordcloud = WordCloud(\n",
    "# # #         background_color='black',\n",
    "# # #         stopwords=stopwords,\n",
    "# # #         max_words=10000,\n",
    "# # #         max_font_size=40, \n",
    "# # #         scale=3,\n",
    "# # #         random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "# # # ).generate(str(data))\n",
    "\n",
    "# # #     fig = plt.figure(1, figsize=(15, 15))\n",
    "# # #     plt.axis('off')\n",
    "# # #     if title: \n",
    "# # #         fig.suptitle(title, fontsize=20)\n",
    "# # #         fig.subplots_adjust(top=2.3)\n",
    "\n",
    "# # #     plt.imshow(wordcloud)\n",
    "# # #     plt.show()\n",
    "\n",
    "# # # show_wordcloud(df_train['tweet'],'Most Common Words from the whole corpus')\n",
    "\n",
    "# # show_wordcloud(df_train[df_train['sentiment'] == 0]['tweet'],'Negative Tweets')\n",
    "\n",
    "# # # df_train['tweet']\n",
    "# # show_wordcloud(df_train[df_train['sentiment'] == 1]['tweet'],'Neutral Tweets')\n",
    "\n",
    "# # show_wordcloud(df_train[df_train['sentiment'] == 2]['tweet'],'Positive Tweets')\n",
    "\n",
    "# # show_wordcloud(df_train[df_train['sentiment'] == 3]['tweet'],'Can\\'t say Tweets')\n",
    "\n",
    "# df_train.iloc[500:600].head()\n",
    "\n",
    "# df_train.isna().sum()\n",
    "\n",
    "# df_train['tweet'].dtype\n",
    "\n",
    "# # df_train['text_length'] = df_train['tweet'].apply(len)\n",
    "# # df_train[['tweet_id','text_length','tweet']].head()\n",
    "\n",
    "# df_train[ df_train['sentiment'] == 0 ].head()\n",
    "\n",
    "# df_train.dropna(inplace=True)\n",
    "\n",
    "# # train_pos = df_train[ df_train['sentiment'] == 2 ]\n",
    "# # train_pos = train_pos['tweet']\n",
    "# # train_neg = df_train[ df_train['sentiment'] == 0]\n",
    "# # train_neg = train_neg['tweet']\n",
    "\n",
    "# # def wordcloud_draw(data, color = 'black'):\n",
    "# #     words = ' '.join(data)\n",
    "# #     cleaned_word = \" \".join([word for word in words.split()\n",
    "# #                             if 'http' not in word\n",
    "# #                                 and not word.startswith('@')\n",
    "# #                                 and not word.startswith('#')\n",
    "# #                                 and not word.startswith('{')\n",
    "# #                                 and word != 'RT'\n",
    "# #                                 and word != 'quot'\n",
    "# #                                 and word != 'amp'\n",
    "# #                             ])\n",
    "# #     wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "# #                       background_color=color,\n",
    "# #                       width=2500,\n",
    "# #                       height=2000\n",
    "# #                      ).generate(cleaned_word)\n",
    "# #     plt.figure(1,figsize=(13, 13))\n",
    "# #     plt.imshow(wordcloud)\n",
    "# #     plt.axis('off')\n",
    "# #     plt.show()\n",
    "    \n",
    "# # print(\"Positive tweets\")\n",
    "# # # wordcloud_draw(train_pos,'white')\n",
    "# # print(\"Negative tweets\")\n",
    "# # # wordcloud_draw(train_neg)\n",
    "# # print('most common words from the corpus')\n",
    "# # wordcloud_draw(df_train['tweet'])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# # # pip install plotly==4.7.1\n",
    "\n",
    "# # import plotly.graph_objects as go\n",
    "\n",
    "# # # fig = go.Figure(go.Funnelarea(\n",
    "# # #     text =df_train.sentiment,\n",
    "# # #     values = df_train.tweet,\n",
    "# # #     title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n",
    "# # #     ))\n",
    "# # # fig.show()\n",
    "\n",
    "# # from collections import Counter\n",
    "\n",
    "# # # df_train['temp_list'] = df_train['tweet'].apply(lambda x:str(x).split())\n",
    "# # # top = Counter([item for sublist in df_train['tweet'] for item in sublist])\n",
    "# # # temp = pd.DataFrame(top.most_common(20))\n",
    "# # # temp.columns = ['Common_words','count']\n",
    "# # # temp.style.background_gradient(cmap='Blues')\n",
    "\n",
    "# # # fig = df_train['tweet'].bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n",
    "# # #              width=700, height=700,color='Common_words')\n",
    "# # # fig.show()\n",
    "\n",
    "# # # def remove_stopword(x):\n",
    "# # #     return [y for y in x if y not in stopwords.words('english')]\n",
    "# # # df_train['tweet_stopwords'] = df_train['tweet'].apply(lambda x:remove_stopword(x))\n",
    "\n",
    "# # # top = Counter([item for sublist in train['temp_list'] for item in sublist])\n",
    "# # # temp = pd.DataFrame(top.most_common(20))\n",
    "# # # temp = temp.iloc[1:,:]\n",
    "# # # temp.columns = ['Common_words','count']\n",
    "# # # temp.style.background_gradient(cmap='Purples')\n",
    "\n",
    "# # # fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\n",
    "# # # fig.show()\n",
    "\n",
    "# # raw_text = [word for word_list in df_train[['tweet']] for word in word_list]\n",
    "\n",
    "\n",
    "# # def words_unique(sentiments,numwords,raw_words):\n",
    "# #     '''\n",
    "# #     Input:\n",
    "# #         segment - Segment category (ex. 'Neutral');\n",
    "# #         numwords - how many specific words do you want to see in the final result; \n",
    "# #         raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n",
    "# #     Output: \n",
    "# #         dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n",
    "\n",
    "# #     '''\n",
    "# #     allother = []\n",
    "# #     for item in df_train[df_train.sentiment != sentiments]['tweet']:\n",
    "# #         for word in item:\n",
    "# #             allother .append(word)\n",
    "# #     allother  = list(set(allother ))\n",
    "    \n",
    "# #     specificnonly = [x for x in raw_text if x not in allother]\n",
    "    \n",
    "# #     mycounter = Counter()\n",
    "    \n",
    "# #     for item in df_train[df_train.sentiment == sentiments]['tweet']:\n",
    "# #         for word in item:\n",
    "# #             mycounter[word] += 1\n",
    "# #     keep = list(specificnonly)\n",
    "    \n",
    "# #     for word in list(mycounter):\n",
    "# #         if word not in keep:\n",
    "# #             del mycounter[word]\n",
    "    \n",
    "# #     Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n",
    "    \n",
    "# #     return Unique_words\n",
    "\n",
    "# # # Unique_Positive= words_unique(2, 20, raw_text)\n",
    "# # # print(\"The top 20 unique words in Positive Tweets are:\")\n",
    "# # # Unique_Positive.style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alphanumeric=['' for i in range(7274)]\n",
    "\n",
    "# # for i in range(7274):\n",
    "# #     a_string = df_train['tweet'][i]\n",
    "# #     if type(a_string)!=float:\n",
    "# #         for character in a_string:\n",
    "# #             if character.isalnum():\n",
    "# #                 alphanumeric[i] += character\n",
    "# #             elif character ==\" \":\n",
    "# #                 alphanumeric[i] +=character\n",
    "# # alphanumeric\n",
    "\n",
    "# # df_train.head()\n",
    "\n",
    "# cdf['tweet'] = alphanumeric\n",
    "\n",
    "# cdf.head()\n",
    "\n",
    "# # Creating Tokens\n",
    "# # tokenized_sents = [word_tokenize(i) for i in alphanumeric]\n",
    "# # tokenized_sents\n",
    "\n",
    "# ## Bag of Words\n",
    "\n",
    "#  - It is a method to extract features from text documents.\n",
    "#  - These features can be used for training ML algorithms.\n",
    "#  - It is basically a vocabulary of all unique words occuring in the document in the training dataset.\n",
    "\n",
    "\n",
    "\n",
    "# # Bow_1 has list of words row wise so like every tweet is converted into a list of words.\n",
    "\n",
    "# bow_1 = cdf['tweet'].apply(lambda x:word_tokenize(str(x)))\n",
    "# # print(bow_1.head())\n",
    "\n",
    "# # Bow_2 has Nested list form of Bow_1 so every list in Bow_1 is combined into one big Nested list.\n",
    "\n",
    "# bow_2 = []\n",
    "# for i in bow_1:\n",
    "#     bow_2.append(i)\n",
    "# # bow_2\n",
    "\n",
    "# # Bow_3 has Nested list flattened into a simple list so all the words used in tweets is present in Bow_3 as an element\n",
    "# # and no nested list present but it has repetition of elements still present.\n",
    "\n",
    "# bow_3 = []\n",
    "# def reemovNestings(l): \n",
    "#     for i in l: \n",
    "#         if type(i) == list: \n",
    "#             reemovNestings(i) \n",
    "#         else: \n",
    "#             bow_3.append(i)\n",
    "# reemovNestings(bow_2)\n",
    "# bow_3\n",
    "\n",
    "# # Bow_final has all unique words of tweets ready to be converted into features for ML Model.\n",
    "\n",
    "# # bow_final = list(set(bow_3))\n",
    "# # bow_final\n",
    "\n",
    "# len(bow_1)\n",
    "\n",
    "# # bow_1\n",
    "\n",
    "# # Lemmi\n",
    "# #initialise word lematizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# for i in range(len(bow_1)):\n",
    "#     print(lemmatizer.lemmatize(bow_1[i]))\n",
    "\n",
    "\n",
    "# # cdf['tweet'].head()\n",
    "\n",
    "# # lemmatizer.lemmatize(cdf['tweet'])\n",
    "\n",
    "# cdf['tweet'] = [lemmatizer.lemmatize(i[:]) for i in cdf['tweet']]\n",
    "\n",
    "# df_train['tweet'].head()\n",
    "\n",
    "# cdf['tweet'].head()\n",
    "\n",
    "# # type(cdf['tweet'][0])\n",
    "\n",
    "# # bow_5 = cdf['tweet'].apply(lambda x:lemmatizer.lemmatize(str(x)))\n",
    "\n",
    "# ps=PorterStemmer()\n",
    "\n",
    "# cdf['tweet_stem'] =  [ps.stem(i[:]) for i in cdf['tweet']]\n",
    "\n",
    "# # cdf['tweet_stem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tweet_id  \\\n",
      "0  1701       \n",
      "1  1851       \n",
      "2  2689       \n",
      "3  4525       \n",
      "4  3604       \n",
      "5  966        \n",
      "6  1395       \n",
      "7  8182       \n",
      "8  8835       \n",
      "9  883        \n",
      "\n",
      "                                                                                                                                             tweet  \\\n",
      "0  #sxswnui #sxsw #apple defining language of touch with different dialects becoming smaller                                                         \n",
      "1  learning ab google doodles! all doodles should be light, funny &amp; innovative, with exceptions for significant occasions. #googledoodle #sxsw   \n",
      "2  one of the most in-your-face ex. of stealing the show in yrs rt @mention &quot;at #sxsw, apple schools the mkt experts&quot;  {link}              \n",
      "3  this iphone #sxsw app would b pretty awesome if it didn't crash every 10mins during extended browsing. #fuckit #illmakeitwork                     \n",
      "4  line outside the apple store in austin waiting for the new ipad #sxsw  {link}                                                                     \n",
      "5  #technews one lone dude awaits ipad 2 at appleûªs sxsw store {link} #tech_news #apple #ipad_2 #sxsw #tablets #tech                               \n",
      "6  sxsw tips, prince, npr videos, toy shopping with zuckerberg.\\r\\n{link}  #sxsw  #ipad                                                              \n",
      "7  nu user rt @mention new #ubersocial for #iphone now in the app store includes uberguide to #sxsw sponsored by #mashable                           \n",
      "8  free #sxsw sampler on itunes {link} #freemusic                                                                                                    \n",
      "9  i think i might go all weekend without seeing the same ipad case twice... #sxsw                                                                   \n",
      "\n",
      "   sentiment  \\\n",
      "0  1           \n",
      "1  1           \n",
      "2  2           \n",
      "3  0           \n",
      "4  1           \n",
      "5  1           \n",
      "6  1           \n",
      "7  1           \n",
      "8  2           \n",
      "9  2           \n",
      "\n",
      "                                                                                                                                    tweet_html  \n",
      "0  #sxswnui #sxsw #apple defining language of touch with different dialects becoming smaller                                                    \n",
      "1  learning ab google doodles! all doodles should be light, funny & innovative, with exceptions for significant occasions. #googledoodle #sxsw  \n",
      "2  one of the most in-your-face ex. of stealing the show in yrs rt @mention \"at #sxsw, apple schools the mkt experts\"  {link}                   \n",
      "3  this iphone #sxsw app would b pretty awesome if it didn't crash every 10mins during extended browsing. #fuckit #illmakeitwork                \n",
      "4  line outside the apple store in austin waiting for the new ipad #sxsw  {link}                                                                \n",
      "5  #technews one lone dude awaits ipad 2 at appleûªs sxsw store {link} #tech_news #apple #ipad_2 #sxsw #tablets #tech                          \n",
      "6  sxsw tips, prince, npr videos, toy shopping with zuckerberg.\\r\\n{link}  #sxsw  #ipad                                                         \n",
      "7  nu user rt @mention new #ubersocial for #iphone now in the app store includes uberguide to #sxsw sponsored by #mashable                      \n",
      "8  free #sxsw sampler on itunes {link} #freemusic                                                                                               \n",
      "9  i think i might go all weekend without seeing the same ipad case twice... #sxsw                                                              \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating copy of the train dataset\n",
    "cdf = df_train.copy()\n",
    "\n",
    "# making tweets to lower case\n",
    "cdf['tweet']  = cdf['tweet'].str.lower()\n",
    "\n",
    "# to remove html tags from the tweets using html praser\n",
    "# Importing HTMLParser\n",
    "from html.parser import HTMLParser\n",
    "html = HTMLParser()\n",
    "\n",
    "# checking for null values\n",
    "cdf['tweet'].isna().sum()\n",
    "\n",
    "# droping the single null value present in the train dataset\n",
    "cdf.dropna(inplace=True)\n",
    "\n",
    "# Created a new columns i.e. tweet_html which do not contain html tags\n",
    "cdf['tweet_html'] = cdf['tweet'].apply(lambda x: html.unescape(x))\n",
    "print(cdf.head(10))\n",
    "\n",
    "type(cdf['tweet_html'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7273, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #sxswnui #sxsw #apple defining language of touch with different dialects becoming smaller                                                  \n",
       "1    learning ab google doodles! all doodles should be light, funny & innovative, with exceptions for significant occasions. #googledoodle #sxsw\n",
       "2    one of the most in-your-face ex. of stealing the show in yrs rt @mention \"at #sxsw, apple schools the mkt experts\"  {link}                 \n",
       "3    this iphone #sxsw app would b pretty awesome if it didn't crash every 10mins during extended browsing. #fuckit #illmakeitwork              \n",
       "4    line outside the apple store in austin waiting for the new ipad #sxsw  {link}                                                              \n",
       "Name: tweet_html, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf['tweet_html'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_html</th>\n",
       "      <th>tweet_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of touch with different dialects becoming smaller</td>\n",
       "      <td>1</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of touch with different dialects becoming smaller</td>\n",
       "      <td>sxswnui sxsw apple defining language touch different dialect becoming smaller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>learning ab google doodles! all doodles should be light, funny &amp;amp; innovative, with exceptions for significant occasions. #googledoodle #sxsw</td>\n",
       "      <td>1</td>\n",
       "      <td>learning ab google doodles! all doodles should be light, funny &amp; innovative, with exceptions for significant occasions. #googledoodle #sxsw</td>\n",
       "      <td>learning ab google doodle doodle light funny innovative exception significant occasion googledoodle sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one of the most in-your-face ex. of stealing the show in yrs rt @mention &amp;quot;at #sxsw, apple schools the mkt experts&amp;quot;  {link}</td>\n",
       "      <td>2</td>\n",
       "      <td>one of the most in-your-face ex. of stealing the show in yrs rt @mention \"at #sxsw, apple schools the mkt experts\"  {link}</td>\n",
       "      <td>one in-your-face ex stealing show yr rt mention sxsw apple school mkt expert '' link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>this iphone #sxsw app would b pretty awesome if it didn't crash every 10mins during extended browsing. #fuckit #illmakeitwork</td>\n",
       "      <td>0</td>\n",
       "      <td>this iphone #sxsw app would b pretty awesome if it didn't crash every 10mins during extended browsing. #fuckit #illmakeitwork</td>\n",
       "      <td>iphone sxsw app would b pretty awesome crash every 10mins extended browsing fuckit illmakeitwork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>line outside the apple store in austin waiting for the new ipad #sxsw  {link}</td>\n",
       "      <td>1</td>\n",
       "      <td>line outside the apple store in austin waiting for the new ipad #sxsw  {link}</td>\n",
       "      <td>line outside apple store austin waiting new ipad sxsw link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id  \\\n",
       "0  1701       \n",
       "1  1851       \n",
       "2  2689       \n",
       "3  4525       \n",
       "4  3604       \n",
       "\n",
       "                                                                                                                                             tweet  \\\n",
       "0  #sxswnui #sxsw #apple defining language of touch with different dialects becoming smaller                                                         \n",
       "1  learning ab google doodles! all doodles should be light, funny &amp; innovative, with exceptions for significant occasions. #googledoodle #sxsw   \n",
       "2  one of the most in-your-face ex. of stealing the show in yrs rt @mention &quot;at #sxsw, apple schools the mkt experts&quot;  {link}              \n",
       "3  this iphone #sxsw app would b pretty awesome if it didn't crash every 10mins during extended browsing. #fuckit #illmakeitwork                     \n",
       "4  line outside the apple store in austin waiting for the new ipad #sxsw  {link}                                                                     \n",
       "\n",
       "   sentiment  \\\n",
       "0  1           \n",
       "1  1           \n",
       "2  2           \n",
       "3  0           \n",
       "4  1           \n",
       "\n",
       "                                                                                                                                    tweet_html  \\\n",
       "0  #sxswnui #sxsw #apple defining language of touch with different dialects becoming smaller                                                     \n",
       "1  learning ab google doodles! all doodles should be light, funny & innovative, with exceptions for significant occasions. #googledoodle #sxsw   \n",
       "2  one of the most in-your-face ex. of stealing the show in yrs rt @mention \"at #sxsw, apple schools the mkt experts\"  {link}                    \n",
       "3  this iphone #sxsw app would b pretty awesome if it didn't crash every 10mins during extended browsing. #fuckit #illmakeitwork                 \n",
       "4  line outside the apple store in austin waiting for the new ipad #sxsw  {link}                                                                 \n",
       "\n",
       "                                                                                               tweet_tokens  \n",
       "0  sxswnui sxsw apple defining language touch different dialect becoming smaller                             \n",
       "1  learning ab google doodle doodle light funny innovative exception significant occasion googledoodle sxsw  \n",
       "2  one in-your-face ex stealing show yr rt mention sxsw apple school mkt expert '' link                      \n",
       "3  iphone sxsw app would b pretty awesome crash every 10mins extended browsing fuckit illmakeitwork          \n",
       "4  line outside apple store austin waiting new ipad sxsw link                                                "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding unnecessary char to stop words\n",
    "stop_words = list(set(stopwords.words('english')))+list(punctuation)+['``', \"'s\", \"...\", \"n't\"]\n",
    "\n",
    "# creating tokens\n",
    "cdf['tweet_tokens'] = [nltk.word_tokenize(x) for x in cdf['tweet_html']]\n",
    "\n",
    "# Removing stop words \n",
    "cdf['tweet_tokens'] = cdf['tweet_tokens'].apply(lambda row: [word for word in row if word not in stop_words])\n",
    "\n",
    "#Applying the stemming\n",
    "\n",
    "# stemmer = PorterStemmer()\n",
    "# cdf['tweet_tokens'] = cdf['tweet_tokens'].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "# cdf['tokenized_tweet'] = cdf['tweet_tokens'].apply(lambda x: ' '.join(x))\n",
    "# cdf.head()\n",
    "\n",
    "#Applying the lemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "cdf['tweet_tokens'] = cdf['tweet_tokens'].apply(lambda x: [lemma.lemmatize(i) for i in x])\n",
    "cdf['tweet_tokens'] = cdf['tweet_tokens'].apply(lambda x: ' '.join(x))\n",
    "cdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data, method = \"tfidf\"):\n",
    "    #arguments: data = all the tweets in the form of array, method = type of feature extracter\n",
    "    #methods of feature extractions: \"tfidf\" and \"doc2vec\"\n",
    "    if method == \"tfidf\":\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        tfv=TfidfVectorizer(sublinear_tf=True) # we need to give proper stopwords list for better performance\n",
    "        features=tfv.fit_transform(data)\n",
    "        \n",
    "    else:\n",
    "        return \"Incorrect inputs\"\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = feature_extraction(cdf['tweet_tokens'],method='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tfidf = vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7273, 8184)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-7ae494669a53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mlog_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_predict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy Score: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f1_score' is not defined"
     ]
    }
   ],
   "source": [
    "#Splitting the dataset into train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(initial_tfidf,cdf['sentiment'],test_size=0.4,random_state=42)\n",
    "\n",
    "#Initialising the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "#Fitting the model with train data\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "#Finding the accuracy score of model on test data\n",
    "acc = log_reg.score(X_test,y_test)\n",
    "log_predict = log_reg.predict(X_test)\n",
    "f1 = f1_score(log_predict,y_test,average='weighted')\n",
    "\n",
    "print (\"Accuracy Score: \", acc)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score ,roc_auc_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train,y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "nb_acc = nb.score(X_test,y_test)\n",
    "f1 = f1_score(y_pred,y_test,average='weighted')\n",
    "\n",
    "print (\"Accuracy Score: \", nb_acc)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_classifier(features, label, classifier = \"logistic_regression\"):\n",
    "#     #arguments: features = output of feature_extraction(...), label = labels in array form, classifier = type of classifier \n",
    "# #     from sklearn.metrics import roc_auc_score # we will use auc as the evaluation metric\n",
    "#     if classifier == \"logistic_regression\": # auc (train data): 0.8780618441250002\n",
    "#         from sklearn.linear_model import LogisticRegression\n",
    "#         model = LogisticRegression(C=1.)\n",
    "#     elif classifier == \"naive_bayes\": # auc (train data): 0.8767891829687501\n",
    "#         from sklearn.naive_bayes import MultinomialNB\n",
    "#         model = MultinomialNB()\n",
    "#     elif classifier == \"svm\": # can't use sklearn svm, as way too much of data so way to slow. have to use tensorflow for svm\n",
    "#         from sklearn.svm import SVC\n",
    "#         model = SVC()\n",
    "#     else:\n",
    "#         print(\"Incorrect selection of classifier\")\n",
    "#     #fit model to data\n",
    "#     model.fit(features, label)\n",
    "#     #make prediction on the same (train) data\n",
    "#     probability_to_be_positive = model.predict_proba(features)[:,1]\n",
    "#     #chcek AUC(Area Undet the Roc Curve) to see how well the score discriminates between negative and positive\n",
    "#     print (\"auc (train data):\" , roc_auc_score(label, probability_to_be_positive))\n",
    "#     #print top 10 scores as a sanity check\n",
    "#     print (\"top 10 scores: \", probability_to_be_positive[:10])\n",
    "#     #print f1 score \n",
    "#     label_pred = model.predict(X_test)\n",
    "#     print('f1 score: ',f1_score(label_pred,label,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_classifier(features=X_train,label=y_train,classifier='naive_bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(text, handle_space=True):\n",
    "\n",
    "    if handle_space:\n",
    "        space = r\"#\\s+\"\n",
    "        text = re.sub(space, \"#\", str(text).lower()).strip()\n",
    "    \n",
    "    hash_tags = r\"#\\S+\"\n",
    "    \n",
    "    return \" \".join(re.findall(hash_tags, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf['hashtags'] = df_train.tweet.apply(get_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['hashtags'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['hashtags'].value_counts().head(10).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash = df_train['tweet'].str.extractall(r\"(#\\S+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdf.to_csv(r'C:\\Users\\Vardhaman\\Desktop\\DS\\Hackathon_3\\git_sentiment\\data\\Iter\\train_clean_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdf.to_csv(r'C:\\Users\\Vardhaman\\Desktop\\DS\\Hackathon_3\\git_sentiment\\data\\Iter\\train_clean_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
